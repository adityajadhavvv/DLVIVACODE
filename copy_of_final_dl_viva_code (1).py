# -*- coding: utf-8 -*-
"""Copy of FINAL DL VIVA CODE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16_u1H7D80uoP7O1vNArsSJWXFR82mQea

## **EXPT 1 ALL GATE**
"""

import numpy as np

# McCulloch-Pitts neuron model implementation
class McCullochPittsNeuron:
    def __init__(self, weights, threshold):
        self.weights = np.array(weights)
        self.threshold = threshold

    def activate(self, inputs):
        weighted_sum = np.dot(inputs, self.weights)
        if weighted_sum >= self.threshold:
            return 1
        else:
            return 0

# Implementing logic gates using McCulloch-Pitts neurons
def AND_gate(inputs):
    weights = [1, 1]
    threshold = 2
    neuron = McCullochPittsNeuron(weights, threshold)
    return neuron.activate(inputs)

def OR_gate(inputs):
    weights = [1, 1]
    threshold = 1
    neuron = McCullochPittsNeuron(weights, threshold)
    return neuron.activate(inputs)

def NOR_gate(inputs):
    weights = [-1, -1]
    threshold = -0.5
    neuron = McCullochPittsNeuron(weights, threshold)
    return neuron.activate(inputs)

def NAND_gate(inputs):
    gate1 = AND_gate(inputs)
    return int(not gate1)

# Testing the logic gates
print("AND Gate:")
print("0 AND 0 =", AND_gate([0, 0]))
print("0 AND 1 =", AND_gate([0, 1]))
print("1 AND 0 =", AND_gate([1, 0]))
print("1 AND 1 =", AND_gate([1, 1]))

print("\nOR Gate:")
print("0 OR 0 =", OR_gate([0, 0]))
print("0 OR 1 =", OR_gate([0, 1]))
print("1 OR 0 =", OR_gate([1, 0]))
print("1 OR 1 =", OR_gate([1, 1]))

print("\nNOR Gate:")
print("0 NOR 0 =", NOR_gate([0, 0]))
print("0 NOR 1 =", NOR_gate([0, 1]))
print("1 NOR 0 =", NOR_gate([1, 0]))
print("1 NOR 1 =", NOR_gate([1, 1]))

print("\nNAND Gate:")
print("0 NAND 0 =", NAND_gate([0, 0]))
print("0 NAND 1 =", NAND_gate([0, 1]))
print("1 NAND 0 =", NAND_gate([1, 0]))
print("1 NAND 1 =", NAND_gate([1, 1]))

"""## **EXPT 2 XOR MULTIPLE **"""

import numpy as np

# Define the unit step function
def unit_step(v):
    return 1 if v >= 0 else 0

# Define the perceptron model
def perceptron(x, w, b):
    v = np.dot(w, x) + b
    return unit_step(v)

# Define the NOT logic function
def NOT_logic(x):
    return perceptron(x, w=-1, b=0.5)

# Define the AND logic function
def AND_logic(x):
    return perceptron(x, w=np.array([1, 1]), b=-1.5)

# Define the OR logic function
def OR_logic(x):
    return perceptron(x, w=np.array([1, 1]), b=-0.5)

# Define the XOR logic function using other logic functions
def XOR_logic(x):
    y1 = AND_logic(x)
    y2 = OR_logic(x)
    y3 = NOT_logic(y1)
    final_x = np.array([y2, y3])
    return AND_logic(final_x)

# Testing the perceptron model
test_cases = [(0, 1), (1, 1), (0, 0), (1, 0)]

for test_case in test_cases:
    x = np.array(test_case)
    print("XOR({}, {}) = {}".format(*test_case, XOR_logic(x)))

"""## **EXPT 3 fully connected deep neural network with at least 1 hidden layers**"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Load the Fashion MNIST dataset
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# Preprocess the data
train_images = train_images / 255.0
test_images = test_images / 255.0

# Define the model
model = Sequential()
model.add(Dense(256, input_shape=(784,), activation="relu"))
model.add(Dense(128, activation="relu"))
model.add(Dense(10, activation="softmax"))

# Compile the model with SGD optimizer
sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
model.compile(optimizer=sgd_optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Display model summary
model.summary()

# Train the model
model.fit(train_images.reshape(-1, 784), train_labels, epochs=10, verbose=1)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(test_images.reshape(-1, 784), test_labels, verbose=0)
print(f'Test accuracy: {test_accuracy}')

# Make predictions
predictions = model.predict(test_images.reshape(-1, 784))

"""## **EXPT 4 GRADIENT DESCENT ALL**"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
# Load the Fashion MNIST dataset
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# Preprocess the data
train_images = train_images / 255.0
test_images = test_images / 255.0

# Define the model
model = Sequential()
model.add(Dense(256, input_shape=(784,), activation="relu"))
model.add(Dense(128, activation="relu"))
model.add(Dense(10, activation="softmax"))

# Compile the model with SGD optimizer
sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
model.compile(optimizer=sgd_optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Display model summary
model.summary()

# Train the model
model.fit(train_images.reshape(-1, 784), train_labels, epochs=5, verbose=1)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(test_images.reshape(-1, 784), test_labels, verbose=0)
print(f'Test accuracy: {test_accuracy}')

# Make predictions
predictions = model.predict(test_images.reshape(-1, 784))


#mini_batch_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9) model.compile(optimizer=mini_batch_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy']))
#momentum_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9) model.compile(optimizer=momentum_optimizer,loss='sparse_categorical_crossentropy', metrics=['accuracy'])
#nesterov_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True) model.compile(optimizer=nesterov_optimizer,loss='sparse_categorical_crossentropy',metrics=['accuracy'])
#adagrad_optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01) model.compile(optimizer=adagrad_optimizer,loss='sparse_categorical_crossentropy',metrics=['accuracy'])
#adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) model.compile(optimizer=adam_optimizer,loss='sparse_categorical_crossentropy',metrics=['accuracy'])

"""## **EXPT 5 FORWARD PASS ONLY**"""

import numpy as np

# Define sigmoid activation function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Input data
input_data = np.array([0.05, 0.1])

# Weights for input to hidden layer
weights_input_hidden = np.array([[0.15, 0.2], [0.25, 0.3]])

# Bias for hidden layer
bias_hidden = np.array([0.35, 0.35])

# Weights for hidden to output layer
weights_hidden_output = np.array([[0.4, 0.45], [0.5, 0.55]])

# Bias for output layer
bias_output = np.array([0.6, 0.6])

# Perform forward pass
hidden_inputs = np.dot(input_data, weights_input_hidden) + bias_hidden
hidden_outputs = sigmoid(hidden_inputs)

output_inputs = np.dot(hidden_outputs, weights_hidden_output) + bias_output
output = sigmoid(output_inputs)

print("Output of the network:", output)

"""## **EXPT 6 CNN with one convolution layer of filter size 8 and filter size 3 and pool size 2**"""

import numpy as np
!pip install mnist
import mnist
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten
from tensorflow.keras.utils import to_categorical
#training & testing
train_images = mnist.train_images()
train_labels = mnist.train_labels()
test_images = mnist.test_images()
test_labels = mnist.test_labels()
# Normalize the images.
train_images = (train_images / 255) - 0.5
test_images = (test_images / 255) - 0.5

# Reshape the images.
train_images = np.expand_dims(train_images, axis=3)
test_images = np.expand_dims(test_images, axis=3)
num_filters = 8
filter_size = 3
pool_size = 2
# Build the model.
model = Sequential([
  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),
  MaxPooling2D(pool_size=pool_size),
  Flatten(),
  Dense(10, activation='softmax'),
])
# Compile the model.
model.compile(optimizer='adam',
  loss='categorical_crossentropy',
  metrics=['accuracy'],
)
# Train the model.
model.fit(
  train_images,
  to_categorical(train_labels),
  epochs=2,
  validation_data=(test_images, to_categorical(test_labels)),
)
model.save_weights('cnn.h5')
predictions = model.predict(test_images[:5])
print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]
print(test_labels[:5]) # [7, 2, 1, 0, 4]

#XOR
# importing Python library
import numpy as np

# define Unit Step Function
def unitStep(v):
	if v >= 0:
		return 1
	else:
		return 0

# design Perceptron Model
def perceptronModel(x, w, b):
	v = np.dot(w, x) + b
	y = unitStep(v)
	return y

# NOT Logic Function
# wNOT = -1, bNOT = 0.5
def NOT_logicFunction(x):
	wNOT = -1
	bNOT = 0.5
	return perceptronModel(x, wNOT, bNOT)

# AND Logic Function
# here w1 = wAND1 = 1,
# w2 = wAND2 = 1, bAND = -1.5
def AND_logicFunction(x):
	w = np.array([1, 1])
	bAND = -1.5
	return perceptronModel(x, w, bAND)

# OR Logic Function
# w1 = 1, w2 = 1, bOR = -0.5
def OR_logicFunction(x):
	w = np.array([1, 1])
	bOR = -0.5
	return perceptronModel(x, w, bOR)

# XOR Logic Function
# with AND, OR and NOT
# function calls in sequence
def XOR_logicFunction(x):
	y1 = AND_logicFunction(x)
	y2 = OR_logicFunction(x)
	y3 = NOT_logicFunction(y1)
	final_x = np.array([y2, y3])
	finalOutput = AND_logicFunction(final_x)
	return finalOutput

# testing the Perceptron Model
test1 = np.array([0, 1])
test2 = np.array([1, 1])
test3 = np.array([0, 0])
test4 = np.array([1, 0])

print("XOR({}, {}) = {}".format(0, 1, XOR_logicFunction(test1)))
print("XOR({}, {}) = {}".format(1, 1, XOR_logicFunction(test2)))
print("XOR({}, {}) = {}".format(0, 0, XOR_logicFunction(test3)))
print("XOR({}, {}) = {}".format(1, 0, XOR_logicFunction(test4)))

#MINIBATCH
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Load the Fashion MNIST dataset
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# Preprocess the data
train_images = train_images / 255.0
test_images = test_images / 255.0

# Define the model architecture
model = Sequential()
model.add(Dense(256, input_shape=(784,), activation="relu"))
model.add(Dense(128, activation="relu"))
model.add(Dense(10, activation="softmax"))

# Compile the model with Mini Batch Gradient Descent optimizer
mini_batch_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)
model.compile(optimizer=mini_batch_optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# Set the batch size
batch_size = 32

# Train the model
history = model.fit(train_images.reshape(-1, 784), train_labels,
                    epochs=2, batch_size=batch_size, verbose=1)

predictions = model.predict(test_images.reshape(-1, 784))

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.datasets import fashion_mnist

# Load the Fashion MNIST dataset
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# Preprocess the data
train_images = train_images / 255.0
test_images = test_images / 255.0

# Define the model architecture
model = Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),  # Flatten the input images
    tf.keras.layers.Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation
    tf.keras.layers.Dense(64, activation='relu'),   # Second hidden layer with 64 neurons and ReLU activation
    tf.keras.layers.Dense(10, activation='softmax') # Output layer with 10 neurons for classification and softmax activation
])

# Compile the model with Adam optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Display the model summary
model.summary()

# Train the model
history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))